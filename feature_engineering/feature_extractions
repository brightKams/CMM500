!pip install python-whois pandas dnspython requests beautifulsoup4

import pandas as pd
import whois
import socket
import requests
from bs4 import BeautifulSoup
from collections import Counter
import math
from math import log2
import re
import glob


# Merge and Clean whois_details for the dataset

def merge_and_clean_csv(file_pattern):
  """Merges multiple CSV files and cleans the data.

  Args:
    file_pattern: A glob pattern to match the CSV files.

  Returns:
    A cleaned DataFrame containing the merged data.
  """

  # Read all CSV files into a list of DataFrames
  dfs = [pd.read_csv(file) for file in glob.glob(file_pattern)]
  
  # Concatenate the DataFrames vertically
  merged_df = pd.concat(dfs, ignore_index=True)


# Clean the data by removing rows with missing values in Error Column
  important_columns = ['Count', 'Domain Name	', 'Registrar', 'Creation Date', 'Expiration Date', 'Name Servers' ]

  # Clean the data by removing rows with missing values
  cleaned_df = merged_df.drop('Error', axis=1) # axis=1 specifies that we're dropping the Error column
  cleaned_df = cleaned_df.drop('Registrar', axis=1) # axis=1 specifies that we're dropping the Registrar column
  cleaned_df = cleaned_df.dropna()

  # Reset the index and renumber the 'Count' column
  cleaned_df = cleaned_df.reset_index(drop=True)  # Reset index to consecutive numbers
  cleaned_df['Count'] = cleaned_df.index + 1  # Renumber 'Count' starting from 1

  return cleaned_df
  # return merged_df


# Replace '*.csv' with the actual pattern of your CSV files
#file_pattern = "./whois_details/result_*.csv"
file_pattern = "result_*.csv"

# Merge and clean the CSV files
cleaned_data = merge_and_clean_csv(file_pattern)

# Save the cleaned data to a new CSV file
cleaned_data.to_csv("merged_cleaned_data.csv", index=False)

display(cleaned_data)




# import pandas as pd
# from collections import Counter





# Load the uploaded dataset after Cleaning it.
malicious_dataset = 'merged_cleaned_data.csv'
malicious_domains = pd.read_csv(malicious_dataset)

# # Display the first few rows to inspect the structure
malicious_domains.head()
df = pd.DataFrame(malicious_domains)

# Function to calculate vowel, consonant, numeric, and other character counts
def calculate_counts(domain):
    vowels = "aeiouAEIOU"
    consonants = "bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ"
    digits = "0123456789"
    
    # domain_length = len(domain)
    vowel_count = sum(1 for char in domain if char in vowels)
    consonant_count = sum(1 for char in domain if char in consonants)
    numeric_count = sum(1 for char in domain if char in digits)
    other_count = sum(1 for char in domain if not char.isalnum())
    
    return vowel_count, consonant_count, numeric_count, other_count

# Function to calculate Shannon entropy
def calculate_entropy(domain):
    # Count frequency of each character
    frequency = Counter(domain)
    total_chars = len(domain)
    
    # Calculate entropy
    entropy = -sum((count / total_chars) * math.log2(count / total_chars) for count in frequency.values())
    return entropy



# Function to scrape HTML element count
# def count_html_elements(domain):
#     try:
#         url = f"http://{domain}"
#         response = requests.get(url, timeout=5)
#         soup = BeautifulSoup(response.text, 'html.parser')
#         return len(soup.find_all())
#     except Exception:
#         return None



# Apply functions to each domain
df["Domain Length"] = df["Domain Name"].apply(len)
df["Vowel Count"] = df["Domain Name"].apply(lambda x: calculate_counts(x)[0])
df["Consonant Count"] = df["Domain Name"].apply(lambda x: calculate_counts(x)[1])
df["Numeric Count"] = df["Domain Name"].apply(lambda x: calculate_counts(x)[2])
df["Other Character Count"] = df["Domain Name"].apply(lambda x: calculate_counts(x)[3])
df["Entropy"] = df["Domain Name"].apply(calculate_entropy)
# df["HTML Element Count"] = df["Domain Name"].apply(count_html_elements)


rearrange_order = df[['Domain Name', 'Domain Length', 'Vowel Count', 'Consonant Count', 'Numeric Count', 
                 'Other Character Count', 'Entropy', 'Creation Date', 'Expiration Date', 
                 'Name Servers']]

rearrange_order.to_csv("features_combination_updated.csv", index=False, header=True)
rearrange_order.head()


