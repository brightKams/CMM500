{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gettext import install\n",
    "\n",
    "\n",
    "# pip install pandas numpy sciki-learn \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"../files/balanced_features.csv\"\n",
    "# file_path = \"test_features.csv\"\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    # Sanitize column names to remove any leading/trailing spaces or unexpected characters\n",
    "    data.columns = data.columns.str.strip()\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    raise ValueError(\"The file is empty.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"An error occurred while loading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and handle them separately for numeric and categorical columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Fill missing values for numeric columns with the mean\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "# Fill missing values for categorical columns with the mode (most frequent value)\n",
    "data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "\n",
    "\n",
    "# # Check for missing values\n",
    "# if data.isnull().sum().any():\n",
    "#     print(\"Dataset contains missing values. Handling missing values...\")\n",
    "#     data = data.fillna(data.mean())  # Replace missing values with column mean\n",
    "\n",
    "# Check for duplicate rows\n",
    "if data.duplicated().any():\n",
    "    print(\"Dataset contains duplicate rows. Removing duplicates...\")\n",
    "    data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "try:\n",
    "    X = data.drop(\"label\", axis=1)  # Assuming the target column is named \"label\"\n",
    "    y = data[\"label\"]\n",
    "except KeyError:\n",
    "    raise KeyError(\"The dataset must contain a 'label' column for classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 4 categorical features...\n",
      "Sparse encoding done. Shape: (75008, 128032)\n",
      "Sparse matrix shape: (75008, 128039)\n",
      "Labels shape: (75008,)\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical features\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "if len(categorical_features) > 0:  # Check if there are any categorical features\n",
    "    print(f\"Encoding {len(categorical_features)} categorical features...\")\n",
    "    encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')  # Use sparse_output\n",
    "    sparse_encoded_data = encoder.fit_transform(X[categorical_features])  # Sparse matrix\n",
    "    print(f\"Sparse encoding done. Shape: {sparse_encoded_data.shape}\")\n",
    "    \n",
    "    # Drop original categorical columns and combine sparse matrix with numerical data\n",
    "    X = X.drop(categorical_features, axis=1)\n",
    "    X = hstack([csr_matrix(X), sparse_encoded_data])\n",
    "\n",
    "\n",
    "    # Apply encoding to the categorical features\n",
    "    encoded_categorical = encoder.fit_transform(data[categorical_features])\n",
    "    \n",
    "    # Convert numerical features to sparse matrix\n",
    "    # numerical_features = data.drop(columns=categorical_features + ['label'], errors='ignore')\n",
    "    # sparse_numerical = csr_matrix(numerical_features.values)\n",
    "        # Drop original categorical columns and prepare numerical data\n",
    "    numerical_data = data.drop(columns=list(categorical_features) + ['label'], errors='ignore')\n",
    "    sparse_numerical = csr_matrix(numerical_data.values)\n",
    "\n",
    "    # Combine encoded categorical and numerical features\n",
    "    X = hstack([sparse_numerical, encoded_categorical])\n",
    "else:\n",
    "    print(\"No categorical features found.\")\n",
    "    # If no categorical features, use only the numerical features\n",
    "\n",
    "    # Combine encoded categorical and numerical features\n",
    "    X = csr_matrix(data.drop(columns=['label'], errors='ignore').values)\n",
    "# The label column\n",
    "y = data['label'].values\n",
    "\n",
    "# Output for verification\n",
    "print(\"Sparse matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler(with_mean=False)  # Use with_mean=False for sparse data\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to try\n",
    "models = {\n",
    "    #\"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"Logistic Regression\": LogisticRegression(solver='saga', max_iter=500, random_state=42, n_jobs=-1),\n",
    "    # \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "    #\"K-Nearest Neighbors\": KNeighborsClassifier(n_jobs=-1),\n",
    "    #\"Support Vector Machine\": SVC(kernel=\"linear\", random_state=42, probability=True),\n",
    "}\n",
    "\n",
    "# Define Support Vector Machine with balanced class weights to handle class imbalance\n",
    "#svm_model = SVC(kernel=\"linear\", random_state=42, probability=True, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ohaka\\Downloads\\CMM500\\cmm500\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression: 0.9028\n",
      "Training time for Logistic Regression: 12.36 seconds\n",
      "Confusion Matrix:\n",
      "[[6488 1013]\n",
      " [ 445 7056]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      7501\n",
      "           1       0.87      0.94      0.91      7501\n",
      "\n",
      "    accuracy                           0.90     15002\n",
      "   macro avg       0.91      0.90      0.90     15002\n",
      "weighted avg       0.91      0.90      0.90     15002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each model\n",
    "results = {} # This line was indented causing the error. Align with for loop.\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "  \n",
    "\n",
    "        # Train the model\n",
    "        # model.fit(X_train_dense, y_train)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train) # Pass the original sparse data for training\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    y_pred = model.predict(X_test) # Pass the original sparse data for prediction\n",
    "    time.sleep(1)\n",
    "        # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy for {model_name}: {accuracy:.4f}\")\n",
    "    print(f\"Training time for {model_name}: {training_time:.2f} seconds\")\n",
    "        # Make predictions\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Store results\n",
    "    results[model_name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "            \"classification_report\": classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "      # except Exception as e:\n",
    "      #     print(f\"An error occurred with {model_name}: {e}\")\n",
    "#\n",
    "# Display best-performing model\n",
    "# best_model = max(results, key=lambda x: results[x][\"accuracy\"])\n",
    "# print(f\"\\nBest Model: {best_model} with Accuracy: {results[best_model]['accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmm500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
